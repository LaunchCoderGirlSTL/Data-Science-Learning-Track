Lesson 10: Linear Regression Part 2
===================================

Learning Objectives
-------------------

* Understanding how multiple features can inform your prediction target
* Gradient descent for multiple variables: simultaneous update of partial derivative of J(theta)
* The reasoning behind feature scaling
* Mean normalization
* Checking the Gradient Descent: automatic convergence test
* Checking the Gradient Descent: adjusting alpha
* Understanding polynomial regression
* Understanding the runtime of normal equation and gradient descent
* Knowing when to use gradient descent and when to use normal equation

Lectures (Videos and Readings)
------------------------------

Complete the readings and videos in the `Linear Regression with Multiple Variables  <https://www.coursera.org/learn/machine-learning>`_ module of Coursera

In Class Assignment
-------------------

We will continue using sklearn to learn about Linear Regression with multiple variables.

Quizzes
-------

Complete the quizzes in the `Linear Regression with Multiple Variables  <https://www.coursera.org/learn/machine-learning>`_ module of Coursera

Homework
--------